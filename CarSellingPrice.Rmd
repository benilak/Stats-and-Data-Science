---
title: "Car Selling Price"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---

### Predicting my vehicle's worth now and in the future:

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, results = FALSE, cache = TRUE,
                      fig.width = 11)
```
```{r}
library(tidyverse)
library(car)
library(pander)

pathfinders <- tibble(price = c(1500, 3995, 3200, 3960, 2995, 3995, 4799, 3795, 
                 3500, 3999, 2995, 2980, 4900, 3990, 3995, 4980,
                 3388, 2990, 3750, 2995, 2495, 2995, 4995, 3995,
                 2492, 2900, 3990, 2900, 1995, 3995, 1995, 8950,
                 1795, 3500, 4500, 1500, 6888, 2002, 3700, 1995,
                 4980, 2995, 1900, 3491, 2900, 3950, 2990, 3990,
                 6888, 3999, 4495, 2492, 4980, 3500, 4799, 2850,
                 4995, 3995, 3500, 3330, 2899, 2900,
                 2895, 2099, 3222, 7289, 4999),
       mileage = c(190000, 151228, 154000, 198670, 204965, 166981, 124717, 126953,
                   214098, 144000, 157900, 216846, 99416, 224195, 224362, 129636,
                   230327, 145377, 179402, 185413, 198970, 199000, 144031, 151374,
                   185888, 159721, 72675, 146989, 177550, 228699, 226127, 37800, 
                   138730, 188000, 53221, 180200, 80167, 168671, 154751, 137001,
                   181669, 204965, 239000, 161818, 187000, 150000, 145377, 224195,
                   80167, 119000, 133718, 185888, 129636, 63221, 124717, 175565,
                   144031, 143652, 154321, 170347, 158897, 159721,
                   121190, 159343, 91789, 59880, 48123))

path.xrecip <- function(x) {2029 + 217500000/x}

pathfinders %>%
  ggplot() +
  geom_point(aes(x = mileage, y = price), 
             color = "darkred", 
             size = 2) +
  geom_abline(aes(intercept = 6255, slope = -.01666, 
                  color = "Original Regression"), 
                  linetype = "dashed") +
  stat_function(fun = path.xrecip, xlim = c(30000, 250000),
                aes(color = "Transformation (X = 1/X)"),
                linetype = "solid",
                size = 1) +
  geom_segment(data = tibble(x = 1:2, y = 1:2),
               aes(x = c(172776, 200000), xend = c(172776, 200000), 
                   y = c(900, 900), yend = c(3288, 3117)),
               color = "darkblue") +
  geom_point(data = tibble(x = c(172776, 200000),
                           y = c(3288, 3117)),
             aes(x = x, y = y),
             color = "darkblue",
             size = 3) +
  annotate("text", x = 172776, y = 1000, 
           label = "172,776", 
           color = "darkblue", size = 3.2, angle = 30,
           vjust = 1.5, hjust = 1) +
  ggrepel::geom_label_repel(data = tibble(x = 1:2, y = 1:2), 
                            aes(x = c(172776, 200000),
                                y = c(3288, 3117)), 
                            label = c("$3288", "$3117"), 
                            color = "darkblue") +
  scale_y_continuous(breaks = seq(2000, 8000, 2000), 
                     labels = scales::dollar,
                     expand = c(0, TRUE)) +
  scale_x_continuous(labels = scales::comma,
                     expand = c(0,0)) +
  coord_cartesian(ylim = c(1000, 9300), clip = "off") +
  labs(title = "Nissan Pathfinder prices and mileage",
       subtitle = "    with added regression lines",
       x = "Mileage",
       y = "Selling Price ($)") +
  scale_color_manual(name = "", values = c("Transformation (X = 1/X)" = "skyblue",
                                           "Original Regression" = "slategray3")) +
  theme_light() +
  theme(panel.grid.minor = element_line(color = "gray95"),
        axis.text.x = element_text(angle = 30, hjust = 1),
        axis.title.y.left = element_text(vjust = 3),
        legend.position = c(.8, .8))

```

$~$

The plot above shows the collected data with a fitted regression curve, both of which can be represented by this equation:

$$
  \underbrace{Y_i}_\text{Price} = 2029 + \frac{217500000}{\underbrace{X_i}_\text{Mileage}} + \epsilon_i \quad \text{where} \quad \epsilon_i \sim N(0,1029^2).
$$

$~$

According to the regression, my vehicle's current predicted price (with 172,776 miles on the engine) is about **$3288**:
$$
2029 + \frac{217500000}{172776} \approx \$3288.
$$

$~$

If I plan to sell my car once it reaches 200,000 miles, the predicted selling price at that time would be **$3117**:
$$
2029 + \frac{217500000}{200000} \approx \$3117.
$$

$~$

The opportunity cost for choosing not to sell my car, calculated as a *cost-per-mile* is approximately **0.628 cents/mile**:
$$
\frac{3117 - 3288}{200000 - 172776} = -\$0.00628
$$

$~$

According to this particular regression, there is no best time to sell in order to minimize the cost-per-mile. This is because the regression line is sloping downwards at an ever decreasing rate, meaning theoretically I can drive my car for an eternity and still be able to sell it for **$2029**:
$$
\lim_{x \rightarrow \infty}(2029 + 217500000/x) = 2029.
$$

In reality this isn't the case - eventually the car will need repairs whose cost exceeds its value. In order to answer this question in a mathematical sense, we would need more data from vehicles that have very large amounts of mileage on the odometer. Likely we would see a downward trend where the selling price approaches close to zero as the mileage increases.

-----

#### Background

I collected this data myself from ksl.com, cars.com, and autotrader.com. I have a 2001 Nissan Pathfinder, and so I limited my search to the same make and model, allowing the 2000, 2001, and 2002 editions. The data can be viewed below.

```{r, results=TRUE}
DT::datatable(pathfinders, options = list(pageLength = 5))
```

-----

#### Technical Details

A simple linear regression was peformed without any transformations of the data. Here is a summary of that regression.

```{r, results=TRUE}
path.lm <- lm(price ~ mileage, data = pathfinders)
pander(summary(path.lm))
```

$~$

I noticed a possible nonlinear trend in the data as well as in a residuals-vs-fitted plot.
```{r}
par(mfrow=c(1,2))
plot(pathfinders$mileage, pathfinders$price)
plot(path.lm, which=1)
```

$~$

A Box-Cox plot suggested either a square-root or log transformation of Y.
```{r}
par(mfrow=c(1,1))
boxCox(path.lm)
```

$~$

But since the residuals-vs-fitted plot showed fairly constant variance, a transformation on X might work too. I tried many transformations, and the best fitting regression came from using the transformation $X = 1/X$.

----

##### Hypotheses
The hypothesis for the regression are:
$$
  H_0: \beta_1 = 0 \\
  H_a: \beta_1 \neq 0
$$

----

##### Checking Assumptions

Conveniently each plot used to check the regression assumptions looked decent enough. 
```{r}
pathfinders.xrecip <- pathfinders %>%
  mutate(mileage = 1/mileage)
path.xrecip.lm <- lm(price ~ mileage, data = pathfinders.xrecip)
plot(path.xrecip.lm, which=1)
```
Even though most of the data is concentrated on the left, the sparse data on the right is still pretty spread out, so I feel safe assuming constant variance.

```{r}
qqPlot(path.xrecip.lm$residuals)
```
The residuals easily pass as normally distributed.

```{r}
plot(path.xrecip.lm$residuals)
```
My only small concern is a slight trumpet shape on the left half of this last plot. But, generally the data is spread out. We assume independent error terms.

----

#### Drawing Conclusions

Here is a summary of the regression after the X-transformation.
```{r, results=TRUE}
pander(summary(path.xrecip.lm))
```

The R-squared value of 0.4334 is better than the original regression's, which was 0.3446. The new fit is definitely an improvement, and is a better fit than any other transformations I tried on both the Y and X variables.

The p-value of $1.41 * 10^{-9}$ is significant, and since our regression assumptions all checked out, we can reject the null hypothesis and conclude that $\beta_1$ is nonzero.

$~$
